{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prabhat Thakur  Date 11/11/2018\n",
    "# MSDS422 - Assignment-6 \n",
    "# Demonstration of Benchmark Experiment using Scikit Learn for Artificial Neural Networks\n",
    "# Utilizes the MNIST data. Completely crossed 3x3 benchmark experiment.\n",
    "\n",
    "# Code reused from:\n",
    "# 4_mnist_from_scratch-data-dump.py - For importing MNIST data.\n",
    "# 6_mnist_from_scratch_scikit-learn-ann-v001.py -\n",
    "# Demonstration of a completely crossed 2x2 benchmark experiment using Scikit Learn to build artificial neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded train-images-idx3-ubyte.gz\n",
      "Already downloaded train-labels-idx1-ubyte.gz\n",
      "Already downloaded t10k-images-idx3-ubyte.gz\n",
      "Already downloaded t10k-labels-idx1-ubyte.gz\n",
      "Extracting /tmp\\train-images-idx3-ubyte.gz\n",
      "Extracting /tmp\\t10k-images-idx3-ubyte.gz\n",
      "Training data shape (60000, 28, 28, 1)\n",
      "Extracting /tmp\\train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp\\t10k-labels-idx1-ubyte.gz\n",
      "Training labels shape (60000, 10)\n",
      "Validation shape (5000, 28, 28, 1)\n",
      "Train size 55000\n",
      "\n",
      "train_data object: <class 'numpy.ndarray'> (55000, 28, 28, 1)\n",
      "train_labels object: <class 'numpy.ndarray'> (55000, 10)\n",
      "validation_data object: <class 'numpy.ndarray'> (5000, 28, 28, 1)\n",
      "validation_labels object: <class 'numpy.ndarray'> (5000, 10)\n",
      "test_data object: <class 'numpy.ndarray'> (10000, 28, 28, 1)\n",
      "test_labels object: <class 'numpy.ndarray'> (10000, 10)\n",
      "\n",
      "data input complete\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "# ensure common functions across Python 2 and 3\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# MNIST from scratch (data and partitioning from Google tensorflow container)\n",
    "# source:  https://hub.docker.com/r/tensorflow/tensorflow/\n",
    "# \n",
    "# We begin with a notebook that walks through an example of training a TensorFlow model \n",
    "# to do digit classification using the [MNIST data set](http://yann.lecun.com/exdb/mnist/). \n",
    "# MNIST is a labeled set of images of handwritten digits.\n",
    "\n",
    "import gzip, binascii, struct, numpy\n",
    "\n",
    "# We'll proceed in steps, beginning with importing and inspecting the MNIST data. This doesn't have anything to do with TensorFlow in particular -- we're just downloading the data archive.\n",
    "import os\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "SOURCE_URL = 'https://storage.googleapis.com/cvdf-datasets/mnist/'\n",
    "WORK_DIRECTORY = \"/tmp\"\n",
    "\n",
    "def maybe_download(filename):\n",
    "    \"\"\"A helper to download the data files if not present.\"\"\"\n",
    "    if not os.path.exists(WORK_DIRECTORY):\n",
    "        os.mkdir(WORK_DIRECTORY)\n",
    "    filepath = os.path.join(WORK_DIRECTORY, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        filepath, _ = urlretrieve(SOURCE_URL + filename, filepath)\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "    else:\n",
    "        print('Already downloaded', filename)\n",
    "    return filepath\n",
    "\n",
    "train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n",
    "train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n",
    "test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n",
    "test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    \n",
    "IMAGE_SIZE = 28\n",
    "PIXEL_DEPTH = 255\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    For MNIST data, the number of channels is always 1.\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        # Skip the magic number and dimensions; we know these values.\n",
    "        bytestream.read(16)\n",
    "\n",
    "        buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images)\n",
    "        data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n",
    "        data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "        data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, 1)\n",
    "        return data\n",
    "\n",
    "train_data = extract_data(train_data_filename, 60000)\n",
    "test_data = extract_data(test_data_filename, 10000)\n",
    "\n",
    "\n",
    "# A crucial difference here is how we `reshape` the array of pixel values. \n",
    "#Instead of one image that's 28x28, we now have a set of 60,000 images, each one being 28x28. We also include a number \n",
    "#of channels, which for grayscale images as we have here is 1.\n",
    "# \n",
    "print('Training data shape', train_data.shape)\n",
    "# Looks good. Now we know how to index our full set of training and test images.\n",
    "\n",
    "# ### Label data\n",
    "# Let's move on to loading the full set of labels. As is typical in classification problems, we'll convert our input labels \n",
    "#into a [1-hot](https://en.wikipedia.org/wiki/One-hot) encoding over a length 10 vector corresponding to 10 digits. \n",
    "#The vector [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], for example, would correspond to the digit 1.\n",
    "\n",
    "NUM_LABELS = 10\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        # Skip the magic number and count; we know these values.\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * num_images)\n",
    "        labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    "    # Convert to dense 1-hot representation.\n",
    "    return (numpy.arange(NUM_LABELS) == labels[:, None]).astype(numpy.float32)\n",
    "\n",
    "train_labels = extract_labels(train_labels_filename, 60000)\n",
    "test_labels = extract_labels(test_labels_filename, 10000)\n",
    "\n",
    "# As with our image data, we'll double-check that our 1-hot encoding of the first few values matches our expectations.\n",
    "print('Training labels shape', train_labels.shape)\n",
    "\n",
    "# The 1-hot encoding looks reasonable.\n",
    "\n",
    "# # ### Segmenting data into training, test, and validation\n",
    "# # The final step in preparing our data is to split it into three sets: training, test, and validation. \n",
    "#This isn't the format of the original data set, so we'll take a small slice of the training data and treat \n",
    "#that as our validation set.\n",
    "VALIDATION_SIZE = 5000\n",
    "\n",
    "validation_data = train_data[:VALIDATION_SIZE, :, :, :]\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "train_data = train_data[VALIDATION_SIZE:, :, :, :]\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "\n",
    "train_size = train_labels.shape[0]\n",
    "print('Validation shape', validation_data.shape)\n",
    "print('Train size', train_size)\n",
    "\n",
    "# check data \n",
    "print('\\ntrain_data object:', type(train_data), train_data.shape)    \n",
    "print('train_labels object:', type(train_labels),  train_labels.shape)  \n",
    "print('validation_data object:', type(validation_data),  validation_data.shape)  \n",
    "print('validation_labels object:', type(validation_labels),  validation_labels.shape)  \n",
    "print('test_data object:', type(test_data),  test_data.shape)  \n",
    "print('test_labels object:', type(test_labels),  test_labels.shape)  \n",
    "\n",
    "print('\\ndata input complete')\n",
    "# End of code from Google mnist_from_scratch program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train object: <class 'numpy.ndarray'> (55000, 784)\n",
      "y_train object: <class 'numpy.ndarray'> (55000,)\n",
      "X_validation object: <class 'numpy.ndarray'> (5000, 784)\n",
      "y_validation object: <class 'numpy.ndarray'> (5000,)\n",
      "X_test object: <class 'numpy.ndarray'> (10000, 784)\n",
      "y_test object: <class 'numpy.ndarray'> (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# user-defined function to convert binary digits to digits 0-9\n",
    "def label_transform(y_in):\n",
    "    for i in range(len(y_in)):\n",
    "        if (y_in[i] == 1): return i\n",
    "\n",
    "y_train = []    \n",
    "for j in range(train_labels.shape[0]):\n",
    "    y_train.append(label_transform(train_labels[j,]))  \n",
    "y_train = np.asarray(y_train)    \n",
    "\n",
    "y_validation = []    \n",
    "for j in range(validation_labels.shape[0]):\n",
    "    y_validation.append(label_transform(validation_labels[j,]))  \n",
    "y_validation = np.asarray(y_validation)    \n",
    "\n",
    "y_test = []    \n",
    "for j in range(test_labels.shape[0]):\n",
    "    y_test.append(label_transform(test_labels[j,]))  \n",
    "y_test = np.asarray(y_test)    \n",
    "    \n",
    "# 28x28 matrix of entries converted to vector of 784 entries    \n",
    "X_train = train_data.reshape(55000, 784)\n",
    "X_validation = validation_data.reshape(5000, 784)    \n",
    "X_test = test_data.reshape(10000, 784)    \n",
    "\n",
    "# check data intended for Scikit Learn input\n",
    "print('\\nX_train object:', type(X_train), X_train.shape)    \n",
    "print('y_train object:', type(y_train),  y_train.shape)  \n",
    "print('X_validation object:', type(X_validation),  X_validation.shape)  \n",
    "print('y_validation object:', type(y_validation),  y_validation.shape)  \n",
    "print('X_test object:', type(X_test),  X_test.shape)  \n",
    "print('y_test object:', type(y_test),  y_test.shape)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train_expanded object: <class 'numpy.ndarray'> (60000, 784)\n",
      "y_train_expanded object: <class 'numpy.ndarray'> (60000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Scikit Learn MLP Classification does validation internally, \n",
    "# so there is with no need for a separate validation set.\n",
    "# We will combine the train and validation sets.\n",
    "\n",
    "X_train_expanded = np.vstack((X_train, X_validation))\n",
    "y_train_expanded = np.vstack((y_train.reshape(55000,1), y_validation.reshape(5000,1)))\n",
    "\n",
    "print('\\nX_train_expanded object:', type(X_train_expanded),  X_train_expanded.shape)  \n",
    "print('y_train_expanded object:', type(y_train_expanded), y_train_expanded.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------\n",
      "\n",
      "Method: ANN-2-Layers-10-Nodes-per-Layer\n",
      "\n",
      "  Specification of method: MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(10, 10), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=9999,\n",
      "       shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.083333, verbose=False, warm_start=False)\n",
      "\n",
      "Processing time (seconds): 73.990584\n",
      "\n",
      "Training set accuracy: 0.934983\n",
      "\n",
      "Test set accuracy: 0.927300\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "Method: ANN-2-Layers-20-Nodes-per-Layer\n",
      "\n",
      "  Specification of method: MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(20, 20), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=9999,\n",
      "       shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.083333, verbose=False, warm_start=False)\n",
      "\n",
      "Processing time (seconds): 111.421004\n",
      "\n",
      "Training set accuracy: 0.966950\n",
      "\n",
      "Test set accuracy: 0.951700\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "Method: ANN-2-Layers-40-Nodes-per-Layer\n",
      "\n",
      "  Specification of method: MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(40, 40), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=9999,\n",
      "       shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.083333, verbose=False, warm_start=False)\n",
      "\n",
      "Processing time (seconds): 70.717736\n",
      "\n",
      "Training set accuracy: 0.989300\n",
      "\n",
      "Test set accuracy: 0.969900\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "Method: ANN-5-Layers-10-Nodes-per-Layer\n",
      "\n",
      "  Specification of method: MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(10, 10, 10, 10, 10), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=9999,\n",
      "       shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.083333, verbose=False, warm_start=False)\n",
      "\n",
      "Processing time (seconds): 83.150943\n",
      "\n",
      "Training set accuracy: 0.943900\n",
      "\n",
      "Test set accuracy: 0.933000\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "Method: ANN-5-Layers-20-Nodes-per-Layer\n",
      "\n",
      "  Specification of method: MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(20, 20, 20, 20, 20), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=9999,\n",
      "       shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.083333, verbose=False, warm_start=False)\n",
      "\n",
      "Processing time (seconds): 68.480915\n",
      "\n",
      "Training set accuracy: 0.964783\n",
      "\n",
      "Test set accuracy: 0.952200\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "Method: ANN-5-Layers-40-Nodes-per-Layer\n",
      "\n",
      "  Specification of method: MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(40, 40, 40, 40, 40), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=9999,\n",
      "       shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.083333, verbose=False, warm_start=False)\n",
      "\n",
      "Processing time (seconds): 95.925517\n",
      "\n",
      "Training set accuracy: 0.992083\n",
      "\n",
      "Test set accuracy: 0.971500\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "Method: ANN-10-Layers-10-Nodes-per-Layer\n",
      "\n",
      "  Specification of method: MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(10, 10, 10, 10, 10, 10, 10, 10, 10, 10),\n",
      "       learning_rate='constant', learning_rate_init=0.001, max_iter=200,\n",
      "       momentum=0.9, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=9999, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.083333, verbose=False, warm_start=False)\n",
      "\n",
      "Processing time (seconds): 90.971020\n",
      "\n",
      "Training set accuracy: 0.930283\n",
      "\n",
      "Test set accuracy: 0.920800\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "Method: ANN-10-Layers-20-Nodes-per-Layer\n",
      "\n",
      "  Specification of method: MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(20, 20, 20, 20, 20, 20, 20, 20, 20, 20),\n",
      "       learning_rate='constant', learning_rate_init=0.001, max_iter=200,\n",
      "       momentum=0.9, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=9999, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.083333, verbose=False, warm_start=False)\n",
      "\n",
      "Processing time (seconds): 91.688528\n",
      "\n",
      "Training set accuracy: 0.966483\n",
      "\n",
      "Test set accuracy: 0.951800\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "Method: ANN-10-Layers-40-Nodes-per-Layer\n",
      "\n",
      "  Specification of method: MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(40, 40, 40, 40, 40, 40, 40, 40, 40, 40),\n",
      "       learning_rate='constant', learning_rate_init=0.001, max_iter=200,\n",
      "       momentum=0.9, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=9999, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.083333, verbose=False, warm_start=False)\n",
      "\n",
      "Processing time (seconds): 101.567178\n",
      "\n",
      "Training set accuracy: 0.983200\n",
      "\n",
      "Test set accuracy: 0.963500\n",
      "\n",
      "Benchmark Experiment: Scikit Learn Artificial Neural Networks\n",
      "\n",
      "                        Method Name  Layers  Nodes per Layer  Processing Time  \\\n",
      "0   ANN-2-Layers-10-Nodes-per-Layer       2               10        73.990584   \n",
      "1   ANN-2-Layers-20-Nodes-per-Layer       2               20       111.421004   \n",
      "2   ANN-2-Layers-40-Nodes-per-Layer       2               40        70.717736   \n",
      "3   ANN-5-Layers-10-Nodes-per-Layer       5               10        83.150943   \n",
      "4   ANN-5-Layers-20-Nodes-per-Layer       5               20        68.480915   \n",
      "5   ANN-5-Layers-40-Nodes-per-Layer       5               40        95.925517   \n",
      "6  ANN-10-Layers-10-Nodes-per-Layer      10               10        90.971020   \n",
      "7  ANN-10-Layers-20-Nodes-per-Layer      10               20        91.688528   \n",
      "8  ANN-10-Layers-40-Nodes-per-Layer      10               40       101.567178   \n",
      "\n",
      "   Training Set Accuracy  Test Set Accuracy  \n",
      "0               0.934983             0.9273  \n",
      "1               0.966950             0.9517  \n",
      "2               0.989300             0.9699  \n",
      "3               0.943900             0.9330  \n",
      "4               0.964783             0.9522  \n",
      "5               0.992083             0.9715  \n",
      "6               0.930283             0.9208  \n",
      "7               0.966483             0.9518  \n",
      "8               0.983200             0.9635  \n"
     ]
    }
   ],
   "source": [
    "# In[4]\n",
    "\n",
    "RANDOM_SEED = 9999\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "names = ['ANN-2-Layers-10-Nodes-per-Layer',\n",
    "         'ANN-2-Layers-20-Nodes-per-Layer',\n",
    "         'ANN-2-Layers-40-Nodes-per-Layer',\n",
    "         'ANN-5-Layers-10-Nodes-per-Layer',\n",
    "         'ANN-5-Layers-20-Nodes-per-Layer',\n",
    "         'ANN-5-Layers-40-Nodes-per-Layer',\n",
    "         'ANN-10-Layers-10-Nodes-per-Layer',\n",
    "         'ANN-10-Layers-20-Nodes-per-Layer',\n",
    "         'ANN-10-Layers-40-Nodes-per-Layer']\n",
    "\n",
    "layers = [2, 2, 2, 5, 5, 5, 10, 10, 10]\n",
    "nodes_per_layer = [10, 20, 40, 10, 20, 40, 10, 20, 40]\n",
    "treatment_condition = [(10, 10), \n",
    "                       (20, 20), \n",
    "                       (40, 40),\n",
    "                       (10, 10, 10, 10, 10), \n",
    "                       (20, 20, 20, 20, 20),\n",
    "                       (40, 40, 40, 40, 40),\n",
    "                       (10, 10, 10, 10, 10, 10, 10, 10, 10, 10), \n",
    "                       (20, 20, 20, 20, 20, 20, 20, 20, 20, 20),\n",
    "                       (40, 40, 40, 40, 40, 40, 40, 40, 40, 40)] \n",
    "\n",
    "# note that validation is included in the method  \n",
    "# for validation_fraction 0.083333, note that 60000 * 0.83333 = 5000    \n",
    "methods = [MLPClassifier(hidden_layer_sizes=treatment_condition[0], activation='relu', \n",
    "              solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \n",
    "              power_t=0.5, max_iter=200, shuffle=True,random_state=RANDOM_SEED, \n",
    "              tol=0.0001, verbose=False, warm_start=False, momentum=0.9,nesterovs_momentum=True,  \n",
    "              early_stopping=False,validation_fraction=0.083333, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
    "    MLPClassifier(hidden_layer_sizes=treatment_condition[1], activation='relu', \n",
    "              solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \n",
    "              power_t=0.5, max_iter=200, shuffle=True, random_state=RANDOM_SEED, \n",
    "              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "              early_stopping=False,validation_fraction=0.083333, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
    "    MLPClassifier(hidden_layer_sizes=treatment_condition[2],activation='relu', \n",
    "              solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \n",
    "              power_t=0.5, max_iter=200, shuffle=True, random_state=RANDOM_SEED, \n",
    "              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "              early_stopping=False,validation_fraction=0.083333, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
    "    MLPClassifier(hidden_layer_sizes=treatment_condition[3], activation='relu', \n",
    "              solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \n",
    "              power_t=0.5, max_iter=200, shuffle=True, random_state=RANDOM_SEED, \n",
    "              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "              early_stopping=False, validation_fraction=0.083333, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
    "    MLPClassifier(hidden_layer_sizes=treatment_condition[4],activation='relu', \n",
    "              solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \n",
    "              power_t=0.5, max_iter=200, shuffle=True, random_state=RANDOM_SEED, \n",
    "              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "              early_stopping=False,validation_fraction=0.083333, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
    "    MLPClassifier(hidden_layer_sizes=treatment_condition[5],activation='relu', \n",
    "              solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \n",
    "              power_t=0.5, max_iter=200, shuffle=True, random_state=RANDOM_SEED, \n",
    "              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "              early_stopping=False,validation_fraction=0.083333, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
    "    MLPClassifier(hidden_layer_sizes=treatment_condition[6],activation='relu', \n",
    "              solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \n",
    "              power_t=0.5, max_iter=200, shuffle=True, random_state=RANDOM_SEED, \n",
    "              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "              early_stopping=False,validation_fraction=0.083333, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
    "    MLPClassifier(hidden_layer_sizes=treatment_condition[7],activation='relu', \n",
    "              solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \n",
    "              power_t=0.5, max_iter=200, shuffle=True, random_state=RANDOM_SEED, \n",
    "              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "              early_stopping=False,validation_fraction=0.083333, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
    "    MLPClassifier(hidden_layer_sizes=treatment_condition[8],activation='relu', \n",
    "              solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \n",
    "              power_t=0.5, max_iter=200, shuffle=True, random_state=RANDOM_SEED, \n",
    "              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "              early_stopping=False,validation_fraction=0.083333, beta_1=0.9, beta_2=0.999, epsilon=1e-08)]\n",
    " \n",
    "index_for_method = 0 \n",
    "training_performance_results = []\n",
    "test_performance_results = []\n",
    "processing_time = []\n",
    "   \n",
    "for name, method in zip(names, methods):\n",
    "    print('\\n------------------------------------')\n",
    "    print('\\nMethod:', name)\n",
    "    print('\\n  Specification of method:', method)\n",
    "    start_time = time.clock()\n",
    "    method.fit(X_train, y_train)\n",
    "    end_time = time.clock()\n",
    "    runtime = end_time - start_time  # seconds of wall-clock time \n",
    "    print(\"\\nProcessing time (seconds): %f\" % runtime)        \n",
    "    processing_time.append(runtime)\n",
    "\n",
    "    # mean accuracy of prediction in training set\n",
    "    training_performance = method.score(X_train_expanded, y_train_expanded)\n",
    "    print(\"\\nTraining set accuracy: %f\" % training_performance)\n",
    "    training_performance_results.append(training_performance)\n",
    "\n",
    "    # mean accuracy of prediction in test set\n",
    "    test_performance = method.score(X_test, y_test)\n",
    "    print(\"\\nTest set accuracy: %f\" % test_performance)\n",
    "    test_performance_results.append(test_performance)\n",
    "                \n",
    "    index_for_method += 1\n",
    "\n",
    "# aggregate the results for final report\n",
    "# using OrderedDict to preserve the order of variables in DataFrame    \n",
    "from collections import OrderedDict  \n",
    "\n",
    "results = pd.DataFrame(OrderedDict([('Method Name', names),\n",
    "                        ('Layers', layers),\n",
    "                        ('Nodes per Layer', nodes_per_layer),\n",
    "                        ('Processing Time', processing_time),\n",
    "                        ('Training Set Accuracy', training_performance_results),\n",
    "                        ('Test Set Accuracy', test_performance_results)]))\n",
    "\n",
    "print('\\nBenchmark Experiment: Scikit Learn Artificial Neural Networks\\n')\n",
    "print(results)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
